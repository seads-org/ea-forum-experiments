{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.dirname(sys.path[0])))\n",
    "\n",
    "from src.paths import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "from pandas import DataFrame\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = Dataset.load_from_disk(dataset_path=datap(\"posts\")).to_pandas()\n",
    "posts[\"postedAt\"] = posts[\"postedAt\"].dt.tz_localize(None)\n",
    "posts = posts.loc[\n",
    "    (posts[\"postedAt\"] >= pd.to_datetime(\"2010-01-01\")) & (posts.body.str.len() > 10)\n",
    "]\n",
    "\n",
    "posts.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import fasttext\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def predict_emotion(text, clf):\n",
    "    em, prob = clf.predict(text)\n",
    "    return em[0].split(\"__\")[-1], prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"finiteautomata/bertweet-base-sentiment-analysis\",\n",
    "    device=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_clf = fasttext.load_model(datap(\"fasttext_empathetic_dialogues.mdl\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data and cache the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text_split import (\n",
    "    extract_paragraphs,\n",
    "    split_long_paragraphs,\n",
    "    collapse_paragraphs_iteratively,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split posts by paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_words = 128\n",
    "posts[\"paragraphs\"] = posts.body.progress_map(extract_paragraphs)\n",
    "posts[\"paragraphs\"] = posts.paragraphs.progress_map(\n",
    "    lambda p: split_long_paragraphs(p, max_n_words=max_n_words)\n",
    ")\n",
    "posts[\"paragraphs_split\"] = posts.paragraphs.progress_map(\n",
    "    lambda x: collapse_paragraphs_iteratively(x, max_n_words=max_n_words)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_split_df = (\n",
    "    posts.explode(column=\"paragraphs_split\")[[\"postedAt\", \"postId\", \"paragraphs_split\"]]\n",
    "    .rename(columns={\"paragraphs_split\": \"text\"})\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "posts_split_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_split_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_split_ds = Dataset.from_pandas(posts_split_df)\n",
    "posts_split_ds.save_to_disk(cachep(\"posts_split_ds\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split comments by paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = Dataset.load_from_disk(dataset_path=datap(\"comments\")).to_pandas()\n",
    "comments = comments.loc[comments.body.str.len() > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_words = 128\n",
    "comments[\"paragraphs\"] = comments.body.progress_map(extract_paragraphs)\n",
    "comments[\"paragraphs\"] = comments.paragraphs.progress_map(\n",
    "    lambda p: split_long_paragraphs(p, max_n_words=max_n_words)\n",
    ")\n",
    "comments[\"paragraphs_split\"] = comments.paragraphs.progress_map(\n",
    "    lambda x: collapse_paragraphs_iteratively(x, max_n_words=max_n_words)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX: this is explode + rename\n",
    "comments_split_df = pd.concat(\n",
    "    [\n",
    "        DataFrame(\n",
    "            {\n",
    "                \"postedAt\": r.postedAt,\n",
    "                \"postId\": r.postId,\n",
    "                \"text\": r.paragraphs_split.text.values,\n",
    "            }\n",
    "        )\n",
    "        for _, r in comments.iterrows()\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "comments_split_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_split_ds = Dataset.from_pandas(comments_split_df)\n",
    "comments_split_ds.save_to_disk(cachep(\"comments_split_ds\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_split_ds = Dataset.from_disk(cachep(\"posts_split_ds\"))\n",
    "posts_split_df = posts_split_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_split_ds = Dataset.load_from_disk(cachep(\"comments_split_ds\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "LABELS = CategoricalDtype([\"POS\", \"NEG\", \"NEU\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sentiment(text):\n",
    "    label = sentiment_classifier(text, truncation=True)[0][\"label\"]\n",
    "    return pd.Categorical([label], categories=LABELS.categories)[0]\n",
    "\n",
    "\n",
    "def simple_sentiment_row(row):\n",
    "    row[\"sentiment\"] = simple_sentiment(row[\"text\"])\n",
    "    return row\n",
    "\n",
    "\n",
    "def simple_sentiment_batch(batch):\n",
    "    texts = batch[\"text\"]\n",
    "    sentiments = sentiment_classifier(texts, truncation=True)\n",
    "    batch[\"sentiment\"] = pd.Series(\n",
    "        pd.Categorical([s[\"label\"] for s in sentiments], categories=LABELS.categories)\n",
    "    )\n",
    "    return batch\n",
    "\n",
    "\n",
    "def simple_emotion_row(row):\n",
    "    text = row[\"text\"].replace(\"\\n\", \"\")\n",
    "    em, prob = predict_emotion(text, emotion_clf)\n",
    "    row[\"emotion\"] = em\n",
    "    row[\"emotion_prob\"] = prob\n",
    "    return row"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = posts_split_ds.map(\n",
    "    simple_sentiment_batch,\n",
    "    batched=True,\n",
    "    # batch_size=2,\n",
    ")\n",
    "\n",
    "result.save_to_disk(cachep(\"posts_split_sentiment_ds\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = posts_split_ds.map(simple_emotion_row)\n",
    "emotions.save_to_disk(cachep(\"posts_split_emotions_ds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "top_emotions = Counter(emotions[\"emotion\"]).most_common(5)\n",
    "top_emotions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_sentiment = comments_split_ds.map(\n",
    "    simple_sentiment_batch,\n",
    "    batched=True,\n",
    "    # batch_size=2,\n",
    ")\n",
    "\n",
    "comments_sentiment.save_to_disk(cachep(\"comments_sentiment_ds\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_emotions = comments_split_ds.map(simple_emotion_row)\n",
    "comments_emotions.save_to_disk(cachep(\"comments_emotions_ds\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a959423663df71e04acaf79cb19bd0368bc06df5eec16525215daedf2c535e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
